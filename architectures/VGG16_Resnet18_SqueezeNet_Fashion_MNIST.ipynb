{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.autograd as ag\n",
    "from utils import mnist_reader\n",
    "import MNISTtools\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from tqdm import tqdm_notebook\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import data_split\n",
    "import gc\n",
    "from utils import utils\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can also add data augmentation transforms here\n",
    "train_transform = transforms.Compose([transforms.Resize(224), \n",
    "                                      transforms.RandomResizedCrop(224),\n",
    "                                      transforms.RandomHorizontalFlip(),\n",
    "                                      transforms.ToTensor(), \n",
    "                                      transforms.Lambda(lambda x: torch.cat([x,x,x],0)) ])\n",
    "\n",
    "# transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))   #transforms.ToPILImage(),\n",
    "test_transform = transforms.Compose([ transforms.Resize(224), \n",
    "                                     transforms.ToTensor(), transforms.Lambda(lambda x: torch.cat([x,x,x],0)) ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Downloading/Checking for data......')\n",
    "trainset = torchvision.datasets.FashionMNIST(root='data/downloads', train=True,\n",
    "                                        download=True, transform=train_transform)       # download=True for the 1st time\n",
    "testset = torchvision.datasets.FashionMNIST(root='data/downloads', train=False,\n",
    "                                        download=True, transform=test_transform)        # download=True for the 1st time\n",
    "train, validation = data_split.train_valid_split(trainset)    # separates 10% for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(train, batch_size, shuffle=True, num_workers=2)\n",
    "validloader = torch.utils.data.DataLoader(validation, batch_size, shuffle=False, num_workers=2)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of batches in training:  5400 ,   Total train data:  54000\n",
      "# of batches in test:  1000 ,   Total test data:  10000\n"
     ]
    }
   ],
   "source": [
    "print('# of batches in training: ',len(trainloader), ',   Total train data: ',len(trainloader)*batch_size)\n",
    "print('# of batches in test: ',len(testloader), ',   Total test data: ',len(testloader)*batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading VGG16 (Batch Normalization) trained on imagenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.vgg16_bn(pretrained='imagenet')\n",
    "\n",
    "#--------------------------------------------------------------------------------------\n",
    "# REMEMBER: vgg16.features --> convolutional layers, vgg16.classifier --> FC layers\n",
    "#--------------------------------------------------------------------------------------\n",
    "\n",
    "# freeze all parameters in covolutional layers\n",
    "for parameter in model.features.parameters():\n",
    "    parameter.require_grad = False\n",
    "\n",
    "in_ftrs = model.classifier[6].in_features\n",
    "features = list(model.classifier.children())[:-1]       # Removing last layer to add out 10 units layer\n",
    "    \n",
    "features.extend([nn.Linear(in_ftrs, 10)])                # adding out layer with 10 units\n",
    "model.classifier = nn.Sequential(*features)             # replacing it with the model with new last layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading  Resnet18 trained on imagenet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.resnet18(pretrained='imagenet')\n",
    "\n",
    "in_ftrs = model.fc.in_features\n",
    "\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading  Squeezenet1_0 trained on imagenet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.squeezenet1_0(pretrained='imagenet')\n",
    "for parameter in model.features.parameters():\n",
    "    parameter.require_grad = False\n",
    "\n",
    "\n",
    "model.classifier = nn.Sequential(\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.Conv2d(512, 10, kernel_size=1),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.AvgPool2d(13)\n",
    ")\n",
    "model.forward = lambda x: model.classifier(model.features(x)).view(x.size(0), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SqueezeNet(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 96, kernel_size=(7, 7), stride=(2, 2))\n",
      "    (1): ReLU(inplace)\n",
      "    (2): MaxPool2d(kernel_size=(3, 3), stride=(2, 2), dilation=(1, 1), ceil_mode=True)\n",
      "    (3): Fire(\n",
      "      (squeeze): Conv2d(96, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU(inplace)\n",
      "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU(inplace)\n",
      "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU(inplace)\n",
      "    )\n",
      "    (4): Fire(\n",
      "      (squeeze): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU(inplace)\n",
      "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU(inplace)\n",
      "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU(inplace)\n",
      "    )\n",
      "    (5): Fire(\n",
      "      (squeeze): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU(inplace)\n",
      "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU(inplace)\n",
      "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU(inplace)\n",
      "    )\n",
      "    (6): MaxPool2d(kernel_size=(3, 3), stride=(2, 2), dilation=(1, 1), ceil_mode=True)\n",
      "    (7): Fire(\n",
      "      (squeeze): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU(inplace)\n",
      "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU(inplace)\n",
      "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU(inplace)\n",
      "    )\n",
      "    (8): Fire(\n",
      "      (squeeze): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU(inplace)\n",
      "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU(inplace)\n",
      "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU(inplace)\n",
      "    )\n",
      "    (9): Fire(\n",
      "      (squeeze): Conv2d(384, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU(inplace)\n",
      "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU(inplace)\n",
      "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU(inplace)\n",
      "    )\n",
      "    (10): Fire(\n",
      "      (squeeze): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU(inplace)\n",
      "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU(inplace)\n",
      "      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU(inplace)\n",
      "    )\n",
      "    (11): MaxPool2d(kernel_size=(3, 3), stride=(2, 2), dilation=(1, 1), ceil_mode=True)\n",
      "    (12): Fire(\n",
      "      (squeeze): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU(inplace)\n",
      "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU(inplace)\n",
      "      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU(inplace)\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.5)\n",
      "    (1): Conv2d(512, 10, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (2): ReLU(inplace)\n",
      "    (3): AvgPool2d(kernel_size=13, stride=13, padding=0, ceil_mode=False, count_include_pad=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, dataloader, compute_loss=False, criterion=None):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    acc_test = 0.0\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    model.train(False)\n",
    "    model.eval()    \n",
    "    for i,data in enumerate(dataloader):\n",
    "        images, labels = data\n",
    "\n",
    "        images = images.cuda()\n",
    "        labels = labels.cuda()\n",
    "\n",
    "        images = ag.Variable(images, volatile=True)\n",
    "        labels = ag.Variable(labels, volatile=True)\n",
    "\n",
    "        outputs = model(images)\n",
    "        \n",
    "        if compute_loss:\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.data[0]\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        acc_test += torch.sum(predicted == labels.data)\n",
    "        total += len(labels.data)\n",
    "        del outputs, predicted, images, labels\n",
    "        torch.cuda.empty_cache()\n",
    "   \n",
    "    return acc_test*1.0/total, total_loss*1.0/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0878, 0.0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(model,testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_file(valid_accs, train_loss_list, valid_loss_list, epoch, best_epoch):\n",
    "    utils.write_list_to_file('assets/validation_accuracies.txt', valid_accs)\n",
    "    utils.write_list_to_file('assets/train_loss_list_epoch_'+str(epoch)+'.txt', train_loss_list)\n",
    "    utils.write_list_to_file('assets/validation_losses.txt', valid_loss_list)\n",
    "    utils.write_list_to_file('assets/best_epoch.txt', [best_epoch])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(True)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "cuda runtime error (2) : out of memory at /opt/conda/conda-bld/pytorch_1523240155148/work/torch/lib/THC/generic/THCStorage.cu:58",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-124-3ff6df1676fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python2.7/site-packages/torch/autograd/variable.pyc\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    165\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \"\"\"\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python2.7/site-packages/torch/autograd/__init__.pyc\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 99\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuda runtime error (2) : out of memory at /opt/conda/conda-bld/pytorch_1523240155148/work/torch/lib/THC/generic/THCStorage.cu:58"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "best_acc=0.0\n",
    "best_model=None\n",
    "best_epoch=1\n",
    "valid_accs=[]\n",
    "valid_loss_list=[]\n",
    "loss_list = []\n",
    "\n",
    "for epoch in range(3):  # loop over the dataset multiple times\n",
    "    model.train(True)\n",
    "    running_loss = 0.0\n",
    "    train_loss_list=[]\n",
    "    \n",
    "    for i, data in enumerate(trainloader, 0):       # 0 is just to start i from 0\n",
    "        \n",
    "        #% of the dataset \n",
    "        #if(i> len(trainloader)/10):\n",
    "        #    break\n",
    "\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "        \n",
    "        inputs = ag.Variable(inputs, requires_grad = False)\n",
    "        labels = ag.Variable(labels, requires_grad = False)\n",
    "        \n",
    "        # transformations\n",
    "        if torch.cuda.is_available():\n",
    "            inputs = inputs.cuda()\n",
    "            labels = labels.cuda()\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        #curr_loss = loss[0].data.cpu().numpy()[0]\n",
    "        #running_loss += curr_loss\n",
    "        running_loss += loss\n",
    "        train_loss_list.append(loss)\n",
    "        \n",
    "        del inputs, labels, loss, outputs\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    valid_acc, avg_valid_loss=test(model, validloader,True, criterion)\n",
    "    valid_accs.append(valid_acc)\n",
    "    valid_loss_list.append(avg_valid_loss)\n",
    "    \n",
    "    if valid_acc > best_acc:                                          # check for best model\n",
    "        best_acc = valid_acc\n",
    "        best_model = copy.deepcopy(model.state_dict())\n",
    "        best_epoch = epoch+1\n",
    "        torch.save(best_model, 'assets/Squeezenet_best_model.pt')      # save best model\n",
    "        \n",
    "    to_file(valid_accs, train_loss_list, valid_loss_list, epoch+1, best_epoch)\n",
    "    loss_list+=train_loss_list\n",
    "    print('epoch: ', epoch+1)\n",
    "    print('valid acc: ', valid_acc)\n",
    "    print('Total time taken in training (secs): ',time.time()-start)\n",
    "print('Total time taken in training (secs): ',time.time()-start)\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t=str(time.strftime(\"%c\"))\n",
    "# utils.write_list_to_file('assets/list_' + str(time.strftime(\"%c\")) + '.txt', l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the model (very important)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'ResNet_100%.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the saved model\n",
    "To load a saved model and test it on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the architecture's skeleton\n",
    "model = torchvision.models.vgg16_bn(pretrained=False)\n",
    "for parameter in model.features.parameters():\n",
    "    parameter.require_grad = False\n",
    "\n",
    "in_ftrs = model.classifier[6].in_features\n",
    "features = list(model.classifier.children())[:-1]       # Removing last layer to add out 10 units layer\n",
    "features.extend([nn.Linear(in_ftrs, 10)])                # adding out layer with 10 units\n",
    "model.classifier = nn.Sequential(*features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.resnet18(pretrained=False)\n",
    "in_ftrs = model.fc.in_features\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load the saved model\n",
    "model.load_state_dict(torch.load('assets/Resnet_best_model.pt'))\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9042, 0.0)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(model, testloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Training Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#plt.subplot(1,2,1)\n",
    "#plt.plot(range(len(loss_list)),loss_list)\n",
    "\n",
    "#plt.xlabel(\"Batches Trained\")\n",
    "#plt.ylabel(\"Loss\")\n",
    "#plt.title(\"Training Loss\")\n",
    "#plt.show()\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "avg_list = []\n",
    "for i in range(1,len(loss_list)):\n",
    "    avg_list.append(sum(loss_list[:i])/(1.0*len(loss_list[:i])))\n",
    "#plt.subplot(1,2,2)\n",
    "\n",
    "plt.plot(range(len(valid_loss_list)),valid_loss_list, 'r')\n",
    "#plt.plot(range(len(valid_loss_list)),[loss_list[i] for i in np.linspace(0,len(loss_list)-1,25, dtype = int)])\n",
    "plt.xlabel(\"Number of Epochs\")\n",
    "plt.ylabel(\"Average Loss\")\n",
    "plt.title(\"Average Validation Loss ResNet-18 (100% of Data)\")\n",
    "plt.show()\n",
    "\n",
    "#sns.regplot(x=\"x\", y=\"f\", data=df1, order=2, ax=ax)\n",
    "#sns.regplot(x=\"x\", y=\"g\", data=df2, order=2, ax=ax2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "[2.7158625, 0.79797953, 0.9465567, 0.53658503, 0.42776138, 0.83123416, 0.59055316, 0.495147, 0.47180885, 0.70226425, 0.5307456, 0.38417262, 0.5232768, 0.58815706, 0.4003051, 0.5211159, 0.3574661, 0.48919147, 0.37042063, 0.42259374, 0.5711557, 0.3930994, 0.38428846, 0.41211215, 0.568676]\n"
     ]
    }
   ],
   "source": [
    "print(len(valid_loss_list))\n",
    "#print(loss_list(np.linspace(0,len(loss_list),25, dtype = int)))\n",
    "print([loss_list[i] for i in np.linspace(0,len(loss_list)-1,25, dtype = int)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(range(len(avg_list)),avg_list)\n",
    "plt.xlabel(\"Batches Trained\")\n",
    "plt.ylabel(\"Average Loss\")\n",
    "plt.title(\"Average Training Loss ResNet-18 (100% of Data)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(valid_accs)),valid_accs, 'g')\n",
    "plt.xlabel(\"Number of Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Validation Set Accuracy ResNet-18 (10% of Data)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
